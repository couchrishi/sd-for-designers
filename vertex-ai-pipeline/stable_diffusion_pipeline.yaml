# PIPELINE DEFINITION
# Name: ethlas-stable-diffusion-training-pipeline
# Description: A pipeline for custom training of the Stable Diffusion model
# Inputs:
#    gcs_class_data_path: str [Default: 'gs://ethlas-customer-1/class']
#    gcs_instance_data_path: str [Default: 'gs://ethlas-customer-1/instance']
#    jobId: str [Default: 'defaultname']
components:
  comp-stable-diffusion-model-deployment-job-op:
    executorLabel: exec-stable-diffusion-model-deployment-job-op
    inputDefinitions:
      parameters:
        jobId:
          parameterType: STRING
        jobName:
          parameterType: STRING
        model_id:
          parameterType: STRING
        task:
          parameterType: STRING
    outputDefinitions:
      parameters:
        endpoint_id:
          parameterType: STRING
        endpoint_name:
          parameterType: STRING
        model:
          parameterType: STRING
  comp-stable-diffusion-training-job-op:
    executorLabel: exec-stable-diffusion-training-job-op
    inputDefinitions:
      parameters:
        gcs_class_data_path:
          parameterType: STRING
        gcs_instance_data_path:
          parameterType: STRING
        gpu_type:
          defaultValue: NVIDIA_TESLA_T4
          isOptional: true
          parameterType: STRING
        jobId:
          parameterType: STRING
        jobName:
          parameterType: STRING
        machine_type:
          defaultValue: n1-highmem-8
          isOptional: true
          parameterType: STRING
        model_id:
          parameterType: STRING
        num_gpus:
          defaultValue: 1.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        num_nodes:
          defaultValue: 1.0
          isOptional: true
          parameterType: NUMBER_INTEGER
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-stable-diffusion-model-deployment-job-op:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - stable_diffusion_model_deployment_job_op
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef stable_diffusion_model_deployment_job_op(\n    model_id: str,\n\
          \    task: str,\n    jobId: str,\n    jobName: str,\n) -> NamedTuple(\"\
          Outputs\", [('model', str), (\"endpoint_name\", str), (\"endpoint_id\",\
          \ str)]):\n\n    # Import statements\n\n    from google.cloud import storage\n\
          \    import os\n    import base64\n    import glob\n    from datetime import\
          \ datetime\n    from io import BytesIO\n    from typing import NamedTuple\n\
          \    import json\n\n    import requests\n    #import torch\n    from google.cloud\
          \ import aiplatform, storage, firestore\n    from PIL import Image\n   \
          \ from google.cloud import storage\n    from google.oauth2 import service_account\n\
          \n    def update_firestore(jobId, job_name, model_deployment_status, model_deployment_endpoint):\n\
          \        \"\"\"Updates Firestore collection with pipeline job details.\"\
          \"\"\n        # Initialize Firestore client\n\n        service_account_key_path\
          \ = 'sai-vertex-sa-key.json'\n        credentials = service_account.Credentials.from_service_account_file(service_account_key_path)\n\
          \        db = firestore.Client(project=\"anthos-multicloud-demo-354204\"\
          , credentials=credentials)\n\n        # Define the document reference\n\
          \        doc_ref = db.collection('vertexPipelineJobs').document(jobId)\n\
          \n        # Update the document fields\n        doc_ref.set({\n        \
          \    \"uploadJobId\": jobId,\n            \"vertexPipelineJobId\": job_name,\n\
          \            \"trainingStatus\": \"completed\",\n            \"modelDeploymentStatus\"\
          : model_deployment_status,\n            \"modelEndpoint\": model_deployment_endpoint\n\
          \        })\n\n    model_name = \"ethlas-stable-diffusion-dreambooth-v1\"\
          \n    #model_name = jobId\n    service_account_key_path = 'sai-vertex-sa-key.json'\n\
          \    credentials = service_account.Credentials.from_service_account_file(service_account_key_path)\n\
          \    endpoint = aiplatform.Endpoint.create(display_name=f\"{jobId}-{task}-endpoint\"\
          , credentials=credentials)\n    SERVICE_ACCOUNT = \"sai-vertex@anthos-multicloud-demo-354204.iam.gserviceaccount.com\"\
          \n    SERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-diffusers-serve\"\
          \n\n    # Serving environment variables\n    serving_env = {\n        \"\
          MODEL_ID\": model_id,\n        \"TASK\": task,\n    }\n\n    model = aiplatform.Model.upload(\n\
          \        display_name=model_name,\n        serving_container_image_uri=SERVE_DOCKER_URI,\n\
          \        serving_container_ports=[7080],\n        serving_container_predict_route=\"\
          /predictions/diffusers_serving\",\n        serving_container_health_route=\"\
          /ping\",\n        serving_container_environment_variables=serving_env,\n\
          \    )\n\n    updated_model_id = model.resource_name.split(\"/\")[-1]\n\n\
          \    # Set deployment_status to \"initiated\"\n    deployment_status = \"\
          Initiated\"\n    deployment_endpoint = \"Not available yet\"\n    update_firestore(jobId,\
          \ jobName, deployment_status, deployment_endpoint)\n\n    try:\n       \
          \ updated_endpoint = model.deploy(\n            endpoint=endpoint,\n   \
          \         machine_type=\"n1-standard-8\",\n            accelerator_type=\"\
          NVIDIA_TESLA_V100\",\n            accelerator_count=1,\n            deploy_request_timeout=1800,\n\
          \            service_account=SERVICE_ACCOUNT,\n        )\n        updated_endpoint_name\
          \ = updated_endpoint.resource_name\n        updated_endpoint_id = updated_endpoint_name.split(\"\
          /\")[-1]\n\n        # Deployment succeeded, update deployment_status to\
          \ \"Completed\"\n        deployment_status = \"Completed\"\n        deployment_endpoint\
          \ = updated_endpoint.resource_name\n\n    except Exception as e:\n     \
          \   # Deployment failed, update deployment_status to \"Failed\" or any other\
          \ appropriate status\n        deployment_status = \"Failed\"\n        deployment_endpoint\
          \ = \"Not available yet\"\n        # Log the error or take any other actions\
          \ as needed\n\n    finally:\n        # Update Firestore collection with\
          \ the final deployment_status, whether it's completed or failed\n      \
          \  update_firestore(jobId, jobName, deployment_status, deployment_endpoint)\n\
          \n    # Return the model and endpoint details\n    return (updated_model_id,\
          \ updated_endpoint_name, updated_endpoint_id)\n\n"
        image: europe-west4-docker.pkg.dev/anthos-multicloud-demo-354204/ethlas-sd-repo/sd-component-image:latest
    exec-stable-diffusion-training-job-op:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - stable_diffusion_training_job_op
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef stable_diffusion_training_job_op(\n    gcs_class_data_path: str,\
          \ \n    gcs_instance_data_path: str,\n    jobId: str,\n    jobName: str,\n\
          \    model_id: str,\n    num_nodes: int = 1,\n    # machine_type: str =\
          \ \"a2-highgpu-1g\",\n    # gpu_type: str = \"NVIDIA_TESLA_A100\",\n   \
          \ machine_type: str = \"n1-highmem-8\",\n    gpu_type: str = \"NVIDIA_TESLA_T4\"\
          ,\n    num_gpus: int = 1\n) -> str:\n\n    # Import statements\n    from\
          \ google.cloud import storage\n    import os\n    import base64\n    import\
          \ glob\n    from datetime import datetime\n    from io import BytesIO\n\
          \    from typing import NamedTuple\n    import json\n\n    import requests\n\
          \    #import torch\n    from google.cloud import aiplatform, storage, firestore\n\
          \    from PIL import Image\n    from google.cloud import storage\n    from\
          \ google.oauth2 import service_account\n\n\n    model_name = \"ethlas-stable-diffusion-dreambooth-v1\"\
          \n    #model_name = jobId\n    service_account_key_path = 'sai-vertex-sa-key.json'\
          \ \n    credentials = service_account.Credentials.from_service_account_file(service_account_key_path)\n\
          \n    INSTANCE_NAME = 'ethlas game avatar' #@param {type:\"string\"} # CHANGE\
          \ THIS ACCORDING TO YOUR IDEAS/THEMES/STYLES\n    CLASS_NAME = '2d game\
          \ avatar' #@param {type:\"string\"} # CHANGE THIS ACCORDING TO YOUR IDEAS/THEMES/STYLES\n\
          \n    REGION = \"europe-west4\"  # @param {type:\"string\"}\n\n    # Create\
          \ three folders under the container's home directory\n    os.makedirs(\"\
          /home/class\", exist_ok=True)\n    os.makedirs(\"/home/instance\", exist_ok=True)\n\
          \    os.makedirs(\"/home/model_output\", exist_ok=True)\n\n    # Set up\
          \ the job_name\n    # prefix = \"ethlas-stable-diffusion\"\n    # user =\
          \ os.environ.get(\"USER\")\n    # now = datetime.now().strftime(\"%Y%m%d_%H%M%S\"\
          )\n    # job_name = f\"{prefix}-{user}-{now}\"\n\n    def update_firestore(jobId,\
          \ job_name, training_status):\n        \"\"\"Updates Firestore collection\
          \ with pipeline job details.\"\"\"\n        # Initialize Firestore client\n\
          \        service_account_key_path = 'sai-vertex-sa-key.json' \n        credentials\
          \ = service_account.Credentials.from_service_account_file(service_account_key_path)\n\
          \        db = firestore.Client(project=\"anthos-multicloud-demo-354204\"\
          , credentials=credentials)\n\n        # Define the document reference\n\
          \        doc_ref = db.collection('vertexPipelineJobs').document(jobId)\n\
          \n        # Update the document fields\n        doc_ref.set({\n        \
          \    \"uploadJobId\": jobId,\n            \"vertexPipelineJobId\": job_name,\n\
          \            \"trainingStatus\": training_status,\n            \"modelDeploymentStatus\"\
          : \"Not initiated yet\",\n            \"modelEndpoint\": \"Not available\
          \ yet\"\n        })\n\n    job_name = jobName\n\n    # Define the necessary\
          \ arguments and parameters for the job\n    args = [\n        \"--method=diffuser_dreambooth\"\
          ,\n        \"--model_name=runwayml/stable-diffusion-v1-5\",\n        \"\
          --input_storage=/gcs/ethlas-customer-1/instance\",\n        \"--output_storage=/gcs/ethlas-customer-1/output_artifacts\"\
          ,\n        #\"--output_storage=/home/model_output\",\n        \"--prompt=a\
          \ 2D ethlas game avatar\",\n        \"--class_prompt=a 2D game avatar\"\
          ,\n        \"--num_class_images=16\",\n        \"--lr=1e-4\",\n        \"\
          --use_8bit=True\",\n        \"--max_train_steps=100\",\n        \"--text_encoder=True\"\
          ,\n        \"--set_grads_to_none=True\"\n    ]\n    #command = [\"python3\"\
          , \"train_wo_nfs.py\"]\n    command = [\"python3\", \"train_wo_nfs.py\"\
          ] + args\n\n    # Build the custom Docker container and push it to the container\
          \ registry (skipping this step)\n\n    # Create the job using aiplatform.CustomContainerTrainingJob\n\
          \    job = aiplatform.CustomContainerTrainingJob(\n        display_name=job_name,\n\
          \        container_uri=\"europe-west4-docker.pkg.dev/anthos-multicloud-demo-354204/ethlas-sd-repo/sd-db-finetuned:latest\"\
          ,\n        command=command,\n        # replica_count=1,\n        # machine_type=\"\
          n1-standard-8\",\n        # accelerator_type=\"NVIDIA_TESLA_T4\",\n    \
          \    # accelerator_count=1,\n        location=REGION,\n        staging_bucket=\"\
          ethlas-customer-1-staging\"\n\n    )\n\n    training_status = \"Initiated\"\
          \n    update_firestore(jobId, jobName, training_status)\n\n    try:\n  \
          \      # Run the job\n        job.run(\n            replica_count=num_nodes,\n\
          \            machine_type=machine_type,\n            accelerator_type=gpu_type,\n\
          \            accelerator_count=num_gpus\n        )\n\n        # Job ran\
          \ successfully, update trainingStatus to \"Completed\"\n        training_status\
          \ = \"Completed\"\n\n    except Exception as e:\n        # Job failed, update\
          \ trainingStatus to \"Failed\" or any other appropriate status\n       \
          \ training_status = \"Failed\"\n        # Log the error or take any other\
          \ actions as needed\n\n    finally:\n        # Update Firestore collection\
          \ with the final training_status, whether it's completed or failed\n   \
          \     update_firestore(jobId, jobName, training_status)\n\n    #model.save(local_model_output_path)\
          \  \n\n    ###########################################################\n\
          \n    #\"\"\"Uploads files in a local directory to a GCS directory.\"\"\"\
          \n\n    # for local_file in glob.glob(local_model_output_path + \"/**\"\
          ):\n    #     if not os.path.isfile(local_file):\n    #         continue\n\
          \    #     filename = local_file[1 + len(local_model_output_path) :]\n \
          \   #     gcs_file_path = os.path.join('gs://ethlas-customer-1/output_artifacts/',\
          \ filename)\n    #     _, blob_name = get_bucket_and_blob_name(gcs_file_path)\n\
          \    #     blob = bucket.blob(blob_name)\n    #     blob.upload_from_filename(local_file)\n\
          \    #     print(\"Copied {} to {}.\".format(local_file, gcs_file_path))\n\
          \n    model_output = \"gs://ethlas-customer-1/output_artifacts\"\n    return\
          \ model_output\n\n"
        image: europe-west4-docker.pkg.dev/anthos-multicloud-demo-354204/ethlas-sd-repo/sd-component-image:latest
        resources:
          accelerator:
            count: '1'
            type: NVIDIA_TESLA_T4
          cpuRequest: 16.0
          memoryRequest: 68.719476736
pipelineInfo:
  description: A pipeline for custom training of the Stable Diffusion model
  name: ethlas-stable-diffusion-training-pipeline
root:
  dag:
    tasks:
      stable-diffusion-model-deployment-job-op:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-stable-diffusion-model-deployment-job-op
        dependentTasks:
        - stable-diffusion-training-job-op
        inputs:
          parameters:
            jobId:
              componentInputParameter: jobId
            jobName:
              runtimeValue:
                constant: ethlas-sd-{{$.inputs.parameters['pipelinechannel--jobId']}}-jupyter-20230730_042833
            model_id:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: stable-diffusion-training-job-op
            pipelinechannel--jobId:
              componentInputParameter: jobId
            task:
              runtimeValue:
                constant: image-to-image
        taskInfo:
          name: stable-diffusion-model-deployment-job-op
      stable-diffusion-training-job-op:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-stable-diffusion-training-job-op
        inputs:
          parameters:
            gcs_class_data_path:
              componentInputParameter: gcs_class_data_path
            gcs_instance_data_path:
              componentInputParameter: gcs_instance_data_path
            jobId:
              componentInputParameter: jobId
            jobName:
              runtimeValue:
                constant: ethlas-sd-{{$.inputs.parameters['pipelinechannel--jobId']}}-jupyter-20230730_042833
            model_id:
              runtimeValue:
                constant: runwayml/stable-diffusion-v1-5
            pipelinechannel--jobId:
              componentInputParameter: jobId
        taskInfo:
          name: stable-diffusion-training-job-op
  inputDefinitions:
    parameters:
      gcs_class_data_path:
        defaultValue: gs://ethlas-customer-1/class
        isOptional: true
        parameterType: STRING
      gcs_instance_data_path:
        defaultValue: gs://ethlas-customer-1/instance
        isOptional: true
        parameterType: STRING
      jobId:
        defaultValue: defaultname
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.0.1
